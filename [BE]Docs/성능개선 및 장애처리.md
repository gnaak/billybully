# [BE] 성능개선 및 장애처리

| :warning: 해당문서는 현재 적용되어 있는것이 아닌, 가상으로 개선하는걸 목적으로 합니다. 또한 Backend Infra에 한정합니다.|
|:------------------------------------------------------|
***

## 들어가기 앞서...
해당 문서는 BillyBully가 대중화가 되어 100만, 1000만 사용자일때 과연 어떤식으로 성능을 개선하고
장애를 해결할지를 가상으로 설명하는 문서임을 밝힙니다.


### 현재의 BillyBully
![BillyBully_Infra](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/BillyBully_Infra.PNG?ref_type=heads)

현재 BillyBully의 Infra Structure 입니다.  
위 사진에서 BE(Backend)부분만 따로 표현을 한다면 다음과 같습니다.

![Backend](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/Backend.PNG?ref_type=heads)
간단히 설명하자면 저희는 Public subnet에서 ec2하나에 nginx돌아가고 있고, Private subnet에 Mysql을 설치해서 사용하고있습니다.

## 조회 성능 개선
만약 1000만명의 사용자를 대상으로 운영하게 된다면 저희서비스에서는 조회 성능을 개선하는 것이 가장큰 이점을 가져다줍니다.  
조회성능을 개선하기 위해서는 두가지 방법이 있습니다.  
### 캐시
첫번째로는 캐시를 적용하는 방법입니다.  
캐시에도 종류가 두가지가 있는데, 글로벌캐시와, 로컬캐시를 비교해보겠습니다.

| 기준 | 글로벌 캐시                                    | 로컬 캐시 |
|------|-------------------------------------------|----------|
| **정의** | 네트워크를 통해 연결된 별도의 캐시 서버에 데이터를 저장합니다.       | 애플리케이션이 실행되고 있는 같은 시스템의 메모리에 데이터를 저장합니다. |
| **접근 속도** | 네트워크 지연으로 인해 로컬 캐시보다 상대적으로 느릴 수 있습니다.     | 매우 빠른 접근 속도를 제공합니다. 네트워크 지연이 없기 때문입니다. |
| **데이터 공유** | 여러 애플리케이션 또는 서비스 간에 데이터를 공유하기 용이합니다.      | 데이터 공유가 기본적으로 지원되지 않습니다. 각 인스턴스 또는 프로세스는 자체 로컬 캐시를 가집니다. |
| **스케일링** | 캐시 서버를 추가하여 용량을 쉽게 확장할 수 있습니다.            | 애플리케이션이 실행되는 각 서버 또는 인스턴스에서 독립적으로 관리되므로, 애플리케이션의 스케일링이 캐시 용량 확장에 직접적인 영향을 미칩니다. |
| **유지 보수** | 네트워크 구성, 캐시 서버의 관리 및 모니터링이 필요합니다.         | 애플리케이션 코드 내에서 관리되므로 별도의 유지 보수가 거의 필요하지 않습니다. |
| **사용 사례** | 대규모 분산 시스템, 마이크로서비스 아키텍처에서 선호됩니다.         | 단일 애플리케이션 또는 서버 내에서 빠른 데이터 액세스가 필요한 경우에 적합합니다. |
| **예시** | Redis, Memcached 같은 분산 캐싱 시스템이 여기에 해당합니다. | 자바의 Ehcache, 구글의 Guava 캐시 등이 로컬 캐싱을 제공합니다. |

> 저희는 1000만 사용자를 가정으로 하고 서버를 분리해 나갈 예정이니, 대규모 시스템에 더 유리한 글로벌 캐시를 사용하겠습니다.

### DB 이분화
두번째로는 DB를 Replication하여 Read연산과 Write 연산을 나눠서 처리하여 성능을 향상시키는 방법입니다.  
**MySQL 이중화 방안은 크게 MMM, MHA로 여기서는 생략하겠습니다.**

### 조회 성능을 개선한 Infra Structure
![성능개선_infra](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/%EC%84%B1%EB%8A%A5%EA%B0%9C%EC%84%A0_infra.PNG?ref_type=heads)
위 그림처럼 캐시를 적용하고 DB를 이분화 한다면 조회성능을 향상시킬수 있을것입니다.

## 안정성과 확장성 (장애 대응)
1000만 명의 사용자를 관리하기 위해서는 성능 향상만큼이나 중요한 것이 시스템이 방대한 사용자를 안정적으로 수용하고 효율적으로 운영할 수 있는 능력입니다. 이를 달성하기 위해, 인프라 구조 내 `단일 장애 지점(SPOF)`을 찾아내고 이를 해결해보겠습니다.

저희 현재 인프라에서 단일장애지점은 Public Subnet에 있는 EC2라고 할 수 있습니다. 단일로 된 EC2에서 모든 서비스를 처리하고 있기때문에, 해당 EC2가 먹통이 된다면 전체 시스템에 영향이 가는 구조입니다.
![단일장애지점](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/%EB%8B%A8%EC%9D%BC%EC%9E%A5%EC%95%A0%EC%A7%80%EC%A0%90.PNG?ref_type=heads)

이러한 단일 장애 지점을 해결하기 위한 최고의 전략은 `서버의 다중화`입니다.
서버를 다중화하여 적용을 하면 아래와 같은 그림이 됩니다.  

![ec2다중화](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/ec2%EB%8B%A4%EC%A4%91%ED%99%94.PNG?ref_type=heads)

시스템의 다중화를 통해 안정성과 확장성을 개선하였음에도 불구하고, 이러한 인스턴스들을 효율적으로 관리하고 부하를 균등하게 분산시킬 외부 서버의 부재가 새로운 문제로 대두되었습니다. 이 문제를 해결하기 위해, 우리는 엔진엑스(Nginx)를 인스턴스 내부가 아닌 외부에 배치하여 다중 인스턴스 간의 부하를 분산할 수 있는 구성을 선택했습니다. 이 구성을 통해 엔진엑스는 단순한 리버스 프록시 역할을 넘어서 로드 밸런서의 기능도 수행하게 됩니다.

이 접근 방식은 트래픽이 급증하는 상황에서도 서비스의 안정성을 유지하고, 모든 사용자 요청을 효과적으로 처리할 수 있는 능력을 강화합니다. 엔진엑스를 사용하여 외부에서 다중 인스턴스를 관리함으로써, 시스템 전반의 부하 분산을 최적화하고, 서비스의 가용성과 성능을 개선할 수 있습니다.
그럼 그림으로 바뀐 구조를 확인해보겠습니다.
![ec2다중화](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/ec2%EB%8B%A4%EC%A4%91%ED%99%94(2).PNG?ref_type=heads)

하지만 이러한 구조에도 문제는 여전히 존재합니다. 문제상황을 살펴보겠습니다.
![ec2다중화(2)](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/%EB%8B%A4%EC%A4%91%ED%99%94%EC%9E%A5%EC%95%A0.PNG?ref_type=heads)
NGINX가 관리 중인 EC2 인스턴스에 장애가 발생했을 경우를 가정해 봅시다. 장애 발생 후, 우리는 빠르게 새로운 인스턴스를 생성하여 문제를 해결했습니다. 하지만, 여기에는 중요한 문제가 남아 있습니다. 바로 NGINX가 새로 생성된 인스턴스의 주소를 자동으로 인식하지 못한다는 점입니다. 이로 인해, NGINX는 요청을 새 인스턴스로 적절히 전달할 수 없게 됩니다.

이러한 상황을 해결하기 위해서는, 인스턴스의 동적 관리와 주소 업데이트 메커니즘을 구축하는 것 외에도, AWS에서 제공하는 Elastic Load Balancer(ELB)를 활용할 수 있습니다. ELB는 자동으로 트래픽을 여러 EC2 인스턴스에 분산시켜 주는 관리형 로드 밸런서로, 인스턴스의 건강 상태를 지속적으로 모니터링하여 장애가 발생한 인스턴스로부터 자동으로 트래픽을 분리합니다.

우리는 ELB를 적용하여 문제를 해결하기로 결정했습니다. ELB의 도입으로, 새로 생성된 EC2 인스턴스의 주소 업데이트와 관리가 자동화되며, 이는 NGINX가 직접 관리해야 하는 부담을 줄여줍니다. 더욱이, ELB의 세밀한 트래픽 분산 및 자동 장애 복구 기능은 우리 시스템의 안정성과 가용성을 대폭 향상시킵니다.

이 접근 방식을 통해, 우리는 장애 발생 시에도 빠르게 대응할 수 있게 되었으며, 사용자 요청을 안정적으로 처리하는 데 필요한 높은 수준의 서비스 지속성을 보장할 수 있게 되었습니다. 또한 ELB에서 제공하는 AutoScaling도 사용 할 수 있게되었습니다.

**변경 후 Infra Structure**
![ELB추가](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/ELB%EC%B6%94%EA%B0%80.PNG?ref_type=heads)
![AutoScaling](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/AutoScaling.PNG?ref_type=heads)

## DB 단일장애지점
현재까지 EC2 인스턴스에 관한 단일장애지점만 설명했지만 사실 저희 서비스에 단일 장애지점은 하나 더 있습니다.
![DB장애지점](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/DB%EC%9E%A5%EC%95%A0%EC%A7%80%EC%A0%90.PNG?ref_type=heads)
이미지에서는 단일 EC2 인스턴스에서 MySQL을 실행하는 것을 볼 수 있습니다. 이 구성은 DB 서버에 장애가 발생하면 전체 시스템이 영향을 받을 수 있는 단일 장애 지점이 됩니다.

![DB장애해결](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/DB%EC%9E%A5%EC%95%A0%ED%95%B4%EA%B2%B0.PNG?ref_type=heads)
MySQL이 여러 EC2 인스턴스에 분산되어 있는 구성을 보여줍니다. 여기에는 쓰기 작업(Write)을 처리하는 'Writer' 인스턴스와 읽기 작업(Read)을 처리하는 여러 'Reader' 인스턴스가 포함되어 있습니다. 이러한 구성은 데이터베이스의 부하를 분산시키고, 한 인스턴스에 장애가 발생해도 다른 인스턴스가 작업을 계속 처리할 수 있게 하여 단일 장애 지점을 제거합니다.

이와 같이 다중화된 데이터베이스 구성을 통해, 우리는 데이터베이스 서버의 장애에 대한 복원력을 향상시켰고, 전체 시스템의 가용성을 높이는 데 성공했습니다. 이는 장애 발생 시 자동 장애 전환(failover)과 빠른 복구 또한 가능하게 합니다.

## 데이터 센터 화재 대응 전략
![데이터센터화재](https://lab.ssafy.com/s10-fintech-finance-sub2/S10P22A401/-/raw/backend/%5BBE%5DDocs/img/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%84%BC%ED%84%B0%ED%99%94%EC%9E%AC.PNG?ref_type=heads)
최근 카카오 데이터 센터에서 발생한 화재 사건은 IT 서비스에 대한 물리적 인프라의 취약성을 일깨워주었습니다. 이 사건을 계기로, 우리 시스템의 재해 복원력과 지속 가능성에 대한 중요성이 강조되었으며, 이에 따라 우리 인프라의 단일 지역에 대한 의존도를 줄이기 위한 조치를 취하게 되었습니다.

제공된 그림을 통해, 우리는 기존 단일 데이터 센터 기반의 인프라를 두 개의 가용 영역(Availability Zones, AZ)으로 확장한 고가용성 구조로 개선한 것을 확인할 수 있습니다. 각 가용 영역은 독립적인 데이터 센터를 이용하여 EC2 인스턴스를 포함한 AWS 자원들이 실행됩니다. 이는 ALB(Amazon Elastic Load Balancer)를 통해 트래픽이 두 지역에 걸쳐 분산되어 처리됨을 의미합니다. Auto Scaling Group은 각 지역의 EC2 인스턴스에 대한 스케일링을 자동으로 관리하며, 한 데이터 센터에 장애가 발생하더라도 다른 지역의 리소스가 서비스를 지속할 수 있도록 합니다.

데이터베이스 또한 동일한 가용 영역 내에서 여러 인스턴스에 걸쳐 복제되어, 한 인스턴스에 문제가 생겼을 때 다른 인스턴스가 그 역할을 대신할 수 있게 됩니다. 이를 통해 데이터의 안전성과 무결성을 보장하며, 장애 발생시 데이터 손실 없이 신속하게 복구할 수 있는 능력을 갖추게 됩니다.

# 결론
BillyBully 서비스가 100만 명, 나아가 1000만 명의 대규모 사용자를 지원하기 위해 필요한 성능 개선 및 장애 대응 전략을 포괄적으로 기술해봤습니다. 전체적으로 저희가 가상이지만 적용해본것을 정리해보며 마무리하겠습니다.

**첫째**, `캐시 전략` 도입을 통해 조회 성능을 크게 향상시켰습니다. 글로벌 캐시를 적용하여 데이터 액세스 시간을 줄이고, 분산된 시스템 환경에서도 효과적으로 데이터를 공유할 수 있도록 만들었습니다.

**둘째**, `데이터베이스 이분화 접근 방식`을 채택하여, 데이터베이스 읽기 및 쓰기 연산의 부하를 분산시켰습니다. 이는 MySQL의 이중화를 통해 실현되었으며, 특히 읽기 연산의 효율성을 대폭 끌어올렸습니다.

**셋째**, `서비스의 장애 대응 능력 강화`를 위해 인프라 내 `단일 장애 지점(SPOF)을 제거`했습니다. 이를 위해 서버 다중화를 실시하고, Nginx와 ELB(Elastic Load Balancer)를 활용하여 부하 분산 및 자동 장애 복구 기능을 통합했습니다. 결과적으로 장애 발생 시 새로운 인스턴스로의 자동 전환을 가능하게 해, 서비스 중단 시간을 현저히 감소시켰습니다.

**넷째**, `데이터 센터의 화재`와 같은 재해에 대비하여 인프라를 두 개의 `독립된 가용 영역`으로 확장했습니다. 이는 전체 시스템의 가용성을 강화하고, 장애 발생 시 빠른 복구를 가능하게 하는 동시에, 지속 가능한 서비스 운영을 보장합니다.
